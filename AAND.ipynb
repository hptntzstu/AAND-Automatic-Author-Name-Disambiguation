{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "import pytest\n",
    "import numpy as np\n",
    "from os.path import join\n",
    "import sys\n",
    "from s2and.data import ANDData\n",
    "from s2and.autofeaturizer import FeaturizationInfo, many_pairs_featurize, featurize, preprocess_features\n",
    "from s2and.consts import LARGE_INTEGER\n",
    "from s2and.eval import pairwise_eval, cluster_eval\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"qian\" # support dataset: aminer, arnetminer, kisti, pubmed, qian, zbmath\n",
    "parent_dir = f\"/{dataset_name}/\" # Your the dataset root\n",
    "dataset = ANDData(\n",
    "    signatures=join(parent_dir, f\"{dataset_name}_signatures.json\"),\n",
    "    papers=join(parent_dir, f\"{dataset_name}_papers.json\"),\n",
    "    mode=\"train\",\n",
    "    specter_embeddings=join(parent_dir, f\"{dataset_name}_specter.pickle\"),\n",
    "    clusters=join(parent_dir, f\"{dataset_name}_clusters.json\"),\n",
    "    block_type=\"s2\",\n",
    "    name=dataset_name,\n",
    "    n_jobs=8,\n",
    "    preprocess=True,\n",
    "    load_name_counts=False,\n",
    "    train_pairs_size = 200000,\n",
    "    val_pairs_size = 20000,\n",
    "    test_pairs_size = 20000,\n",
    "    W2Vmodels=parent_dir + f'{dataset_name}_word2vec',\n",
    "    idf_counts=parent_dir + f'{dataset_name}_counts',\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_use = [\n",
    "    \"affiliation_similarity\",\n",
    "    \"coauthor_similarity\",\n",
    "    \"venue_similarity\",\n",
    "    \"journal_similarity\",\n",
    "    \"title_similarity\",\n",
    "    \"reference_authors_similarity\",\n",
    "    \"reference_titles_similarity\",\n",
    "    \"reference_journals_similarity\",\n",
    "    \"year_diff\",\n",
    "    'misc_features'\n",
    "]\n",
    "featurization_info = FeaturizationInfo(features_to_use=features_to_use)\n",
    "train, val, test = featurize(dataset, featurization_info, n_jobs=8, use_cache=True, all_pair=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, _, params = preprocess_features(train,phase='train')\n",
    "X_test, y_test, _, _ = preprocess_features(test,phase='test',params=params)\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "under_sampler = RandomUnderSampler(random_state=42)\n",
    "X_train, y_train = under_sampler.fit_resample(X_train, y_train)\n",
    "print(f\"Testing target statistics: {Counter(y_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import itertools\n",
    "from sklearn.metrics import roc_curve, auc, f1_score, precision_recall_curve, average_precision_score\n",
    "import copy\n",
    "class LR(nn.Module):\n",
    "    def __init__(self, x_dim):\n",
    "        super(LR,self).__init__()\n",
    "        self.linear1=nn.Linear(x_dim, 32)\n",
    "        self.bn=nn.BatchNorm1d(32)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.dp = nn.Dropout(0.15)\n",
    "        self.linear2=nn.Linear(32, 1)\n",
    "        self.sigmoid=nn.Sigmoid()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=self.linear1(x)\n",
    "        x=self.bn(x)\n",
    "        x=self.relu(x)\n",
    "        x=self.dp(x)\n",
    "        x=self.linear2(x)\n",
    "        x=self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "class feature_selection(nn.Module):\n",
    "    def __init__(self, groups):#[6,7]\n",
    "        super(feature_selection,self).__init__() \n",
    "        self.groups = groups\n",
    "        for i,g in enumerate(groups):\n",
    "            self.register_parameter('fs_group%d'%i, \n",
    "                                    nn.Parameter(1e-3*torch.ones(g[1]-g[0]), requires_grad=True))\n",
    "        self.x_dim = len(groups)\n",
    "       \n",
    "    def forward(self,x):\n",
    "        output_x = []\n",
    "        for i, (g, p) in enumerate(zip(self.groups,self.parameters())):\n",
    "            selected_feature = torch.mm(x[:,g[0]:g[1]], F.gumbel_softmax(p, tau=0.1, hard=True).unsqueeze(1))\n",
    "            output_x.append(selected_feature)\n",
    "        return torch.cat(output_x,dim=1)\n",
    "    \n",
    "    def infer(self,x, manual):\n",
    "        # manual = np.setdiff1d(np.arange(0,self.x_dim),[])\n",
    "        return x[:,manual]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train = X_train[:,:46]\n",
    "X_test = X_test[:,:46]\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1)\n",
    "\n",
    "misc_feature = np.arange(46,X_train.shape[1]).tolist()\n",
    "\n",
    "low_high_group = [[0,6],[6,11],[11,17],[17,23],[23,30],[30,34],[34,38],[38,42],[42,46]]\n",
    "random_feature = [np.random.randint(low,high) for low,high in low_high_group] \n",
    "\n",
    "fs = feature_selection(low_high_group)\n",
    "model = LR(9)\n",
    "proxy_model = LR(9)\n",
    "proxy_model.load_state_dict(model.state_dict())\n",
    "\n",
    "def train():\n",
    "    global y_val\n",
    "    torch.manual_seed(11)\n",
    "    trainset = TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train))\n",
    "    valset = TensorDataset(torch.FloatTensor(X_val), torch.FloatTensor(y_val))\n",
    "    testset = TensorDataset(torch.FloatTensor(X_test), torch.FloatTensor(y_test))\n",
    "    train_loader = DataLoader(trainset, batch_size=100,shuffle=True)\n",
    "    val_loader = DataLoader(valset, batch_size=10,shuffle=True)\n",
    "    test_loader = DataLoader(testset, batch_size=100,shuffle=False)\n",
    "\n",
    "    epoch = 5\n",
    "    loss_fn=nn.BCELoss()\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=2e-4)\n",
    "    optimizer_proxy = torch.optim.SGD(proxy_model.parameters(), lr=0.01, momentum=0.9, weight_decay=2e-4)\n",
    "    optimizer_fs = torch.optim.SGD(fs.parameters(), lr=0.01, momentum=0.9, weight_decay=2e-4)\n",
    "\n",
    "    lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,\n",
    "        milestones=[0.5*epoch,0.75*epoch,0.9*epoch], gamma=0.1)\n",
    "\n",
    "    for i in range(epoch):\n",
    "        model.train() \n",
    "        proxy_model.train()   \n",
    "        for (x, y), (x_val, y_val) in zip(train_loader,val_loader):\n",
    "            proxy_model.load_state_dict(model.state_dict())\n",
    "            model.zero_grad()\n",
    "            proxy_model.zero_grad()\n",
    "\n",
    "            #update proxy\n",
    "            with torch.no_grad():\n",
    "                x_fs = fs(x)\n",
    "            y_pred = proxy_model(x_fs)\n",
    "            loss = loss_fn(y_pred.squeeze(), y)\n",
    "            loss.backward()\n",
    "            optimizer_proxy.step()\n",
    "\n",
    "            #update Gumbel-Softmax\n",
    "            x_val_fs = fs(x_val)\n",
    "            y_pred = proxy_model(x_val_fs)\n",
    "            loss = loss_fn(y_pred.squeeze(), y_val)\n",
    "            loss.backward()\n",
    "            optimizer_fs.step()\n",
    "\n",
    "            #update prediction model\n",
    "            x_fs = fs(x)\n",
    "            y_pred = model(x_fs)\n",
    "            loss = loss_fn(y_pred.squeeze(), y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(fs.state_dict()['fs_group0'])\n",
    "        \n",
    "        model.eval()\n",
    "        pred = model(fs(testset.tensors[0])).detach().numpy()\n",
    "        fpr, tpr, threshold = roc_curve(np.array(y_test,np.int), pred)\n",
    "        aucres = auc(fpr,tpr)\n",
    "        print(\"Epoch:{}  Auc:{}\".format(i, aucres))\n",
    "\n",
    "    return model, aucres\n",
    "train()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9d900c2b60c26ce6318053b421ac0876975ed7b9f831a8634df63f57a997f8a8"
  },
  "kernelspec": {
   "display_name": "Python 3.7.0 ('s2and')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
